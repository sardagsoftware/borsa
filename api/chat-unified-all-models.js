// AILYDIAN ULTRA PRO - Unified AI Chat API
// All 22 AI Models Support - Production Ready
// Model names are hidden from frontend (security + competitive advantage)

require('dotenv').config();
const OpenAI = require('lydian-labs');
const { Anthropic } = require('@anthropic-ai/sdk');
const axios = require('axios');
const { getModelConfig, getActiveModels } = require('./models-config');

// Rate limiting
const requestLog = new Map();
const RATE_LIMIT = 100;
const RATE_WINDOW = 60000;

function checkRateLimit(userId = 'anonymous') {
  const now = Date.now();
  const userRequests = requestLog.get(userId) || [];
  const recentRequests = userRequests.filter(time => now - time < RATE_WINDOW);

  if (recentRequests.length >= RATE_LIMIT) {
    return false;
  }

  recentRequests.push(now);
  requestLog.set(userId, recentRequests);
  return true;
}

/**
 * Calculate confidence score for AI response
 * Based on hedging language, response length, and model tier
 */
function calculateConfidence(response, modelCategory) {
  // Turkish hedging patterns
  const hedgingPatterns = [
    /belki/gi, /muhtemelen/gi, /sanırım/gi, /olabilir/gi,
    /düşünüyorum/gi, /emin değilim/gi, /kesin değil/gi,
    /tahminimce/gi, /galiba/gi, /zannediyorum/gi,
    /possibly/gi, /maybe/gi, /perhaps/gi, /uncertain/gi,
    /might be/gi, /could be/gi, /not sure/gi, /i think/gi,
    /approximate/gi, /roughly/gi
  ];

  // Count hedging instances
  let hedgingCount = 0;
  hedgingPatterns.forEach(pattern => {
    const matches = response.match(pattern);
    if (matches) hedgingCount += matches.length;
  });

  // Base confidence by model category
  const baseConfidence = {
    'ultra': 96,
    'premium': 93,
    'standard': 88,
    'economy': 82,
    'specialized': 90
  };

  const base = baseConfidence[modelCategory] || 88;

  // Reduce for hedging (max -25 points)
  const hedgingPenalty = Math.min(hedgingCount * 4, 25);

  // Response length bonus (longer detailed responses are often more reliable)
  const lengthBonus = response.length > 500 ? 3 : (response.length > 200 ? 1 : 0);

  // Calculate final confidence (60-99 range)
  const confidence = Math.max(60, Math.min(99, base - hedgingPenalty + lengthBonus));

  // Determine confidence level
  let level = 'high';
  if (confidence < 70) level = 'low';
  else if (confidence < 82) level = 'medium';

  return {
    percentage: confidence,
    level,
    isUncertain: confidence < 75,
    suggestResearch: confidence < 70
  };
}

// System prompt - multilingual + hidden model names
const getSystemPrompt = () => {
  return {
    role: 'system',
    content: `You are Ailydian AI, an advanced multilingual AI assistant.

**CRITICAL RULES:**
1. ALWAYS respond in the SAME language as the user's question
2. NEVER reveal which AI model you are powered by
3. Always identify yourself as "Ailydian AI"
4. Provide detailed, professional, and helpful responses
5. Use proper Markdown formatting

**TURKISH:**
- Türkçe soru → MUTLAKA Türkçe cevap
- Detaylı ve profesyonel yanıtlar

**ENGLISH:**
- English question → Respond in English
- Detailed and professional answers

**OTHER LANGUAGES:**
- Detect language and respond in same language

You are Ailydian AI - a premium AI assistant.`
  };
};

// OpenAI-compatible API call (for OpenAI, Groq, Mistral, etc.)
async function callOpenAICompatibleAPI(config, messages, max_tokens, temperature) {
  const client = new OpenAI({
    apiKey: config.apiKey,
    baseURL: config.endpoint
  });

  const completion = await client.chat.completions.create({
    model: config.model,
    messages: messages,
    max_tokens: Math.min(max_tokens, config.maxTokens),
    temperature: Math.max(0, Math.min(2, temperature))
  });

  return {
    response: completion.choices[0].message.content,
    usage: {
      prompt_tokens: completion.usage.prompt_tokens,
      completion_tokens: completion.usage.completion_tokens,
      total_tokens: completion.usage.total_tokens
    }
  };
}

// Anthropic AX9F7E2B API call
async function callAnthropicAPI(config, messages, max_tokens, temperature) {
  const client = new Anthropic({
    apiKey: config.apiKey
  });

  // Separate system messages
  const systemMessage = messages.find(m => m.role === 'system');
  const chatMessages = messages.filter(m => m.role !== 'system');

  const completion = await client.messages.create({
    model: config.model,
    messages: chatMessages,
    system: systemMessage?.content,
    max_tokens: Math.min(max_tokens, config.maxTokens),
    temperature: Math.max(0, Math.min(1, temperature))
  });

  return {
    response: completion.content.map(block => block.type === 'text' ? block.text : '').join(''),
    usage: {
      prompt_tokens: completion.usage.input_tokens,
      completion_tokens: completion.usage.output_tokens,
      total_tokens: completion.usage.input_tokens + completion.usage.output_tokens
    }
  };
}

// Google Gemini API call
async function callGeminiAPI(config, messages, max_tokens, temperature) {
  const systemMessage = messages.find(m => m.role === 'system');
  const chatMessages = messages.filter(m => m.role !== 'system');

  // Convert to Gemini format
  const contents = chatMessages.map(m => ({
    role: m.role === 'assistant' ? 'model' : 'user',
    parts: [{ text: m.content }]
  }));

  const requestBody = {
    contents: contents,
    generationConfig: {
      temperature: Math.max(0, Math.min(2, temperature)),
      maxOutputTokens: Math.min(max_tokens, config.maxTokens)
    }
  };

  if (systemMessage) {
    requestBody.systemInstruction = {
      parts: [{ text: systemMessage.content }]
    };
  }

  const apiUrl = `${config.endpoint}/models/${config.model}:generateContent?key=${config.apiKey}`;

  const response = await axios.post(apiUrl, requestBody, {
    headers: { 'Content-Type': 'application/json' }
  });

  const responseText = response.data.candidates[0].content.parts.map(part => part.text || '').join('');

  return {
    response: responseText,
    usage: {
      prompt_tokens: response.data.usageMetadata?.promptTokenCount || 0,
      completion_tokens: response.data.usageMetadata?.candidatesTokenCount || 0,
      total_tokens: response.data.usageMetadata?.totalTokenCount || 0
    }
  };
}

// Azure OpenAI API call
async function callAzureOpenAIAPI(config, messages, max_tokens, temperature) {
  const client = new OpenAI({
    apiKey: config.apiKey,
    baseURL: config.endpoint,
    defaultQuery: { 'api-version': '2024-08-01-preview' },
    defaultHeaders: { 'api-key': config.apiKey }
  });

  const completion = await client.chat.completions.create({
    model: config.model,
    messages: messages,
    max_tokens: Math.min(max_tokens, config.maxTokens),
    temperature: Math.max(0, Math.min(2, temperature))
  });

  return {
    response: completion.choices[0].message.content,
    usage: {
      prompt_tokens: completion.usage.prompt_tokens,
      completion_tokens: completion.usage.completion_tokens,
      total_tokens: completion.usage.total_tokens
    }
  };
}

// Main request handler
async function handleRequest(req, res) {
  res.setHeader('Access-Control-Allow-Origin', '*');
  res.setHeader('Access-Control-Allow-Methods', 'POST, GET, OPTIONS');
  res.setHeader('Access-Control-Allow-Headers', 'Content-Type, Authorization');

  if (req.method === 'OPTIONS') {
    return res.status(200).end();
  }

  if (req.method === 'GET') {
    // Return available models (without exposing real model names)
    const activeModels = getActiveModels();
    return res.status(200).json({
      success: true,
      models: activeModels.map(m => ({
        id: m.id,
        category: m.category,
        description: m.description,
        // DO NOT expose real model names, apiKey, or endpoint
      })),
      totalModels: activeModels.length
    });
  }

  if (req.method !== 'POST') {
    return res.status(405).json({
      success: false,
      error: 'Method not allowed'
    });
  }

  const userId = req.headers['x-user-id'] || req.ip || 'anonymous';
  if (!checkRateLimit(userId)) {
    return res.status(429).json({
      success: false,
      error: 'Rate limit exceeded'
    });
  }

  try {
    const {
      message,
      messages = [],
      model = 'OX7A3F8D', // default model
      max_tokens = 4096,
      temperature = 0.7
    } = req.body;

    if (!message && !messages.length) {
      return res.status(400).json({
        success: false,
        error: 'Message required'
      });
    }

    // Get model configuration
    const modelConfig = getModelConfig(model);
    if (!modelConfig) {
      return res.status(400).json({
        success: false,
        error: 'Invalid model'
      });
    }

    if (!modelConfig.active || !modelConfig.apiKey) {
      return res.status(503).json({
        success: false,
        error: 'Model not available'
      });
    }

    // Prepare messages
    const systemPrompt = getSystemPrompt();
    const messageArray = messages.length > 0
      ? [systemPrompt, ...messages]
      : [systemPrompt, { role: 'user', content: message }];

    let result;

    // Route to appropriate API based on provider
    switch (modelConfig.provider) {
      case 'lydian-research':
        result = await callAnthropicAPI(modelConfig, messageArray, max_tokens, temperature);
        break;

      case 'lydian-vision':
        result = await callGeminiAPI(modelConfig, messageArray, max_tokens, temperature);
        break;

      case 'azure-openai':
        result = await callAzureOpenAIAPI(modelConfig, messageArray, max_tokens, temperature);
        break;

      case 'lydian-labs':
      case 'lydian-velocity':
      case 'lydian-enterprise':
      case 'deepseek':
      case 'yi':
      case 'zhipu':
      case 'asi':
      case 'z-ai':
      default:
        result = await callOpenAICompatibleAPI(modelConfig, messageArray, max_tokens, temperature);
        break;
    }

    // Calculate confidence score
    const confidence = calculateConfidence(result.response, modelConfig.category);

    // Return response WITHOUT revealing model name
    res.status(200).json({
      success: true,
      response: result.response,
      model: model, // Return user-requested model ID (not real model name)
      provider: 'Ailydian AI', // Generic provider name
      category: modelConfig.category,
      usage: result.usage,
      confidence: confidence,
      timestamp: new Date().toISOString()
    });

  } catch (error) {
    console.error('❌ Unified AI Error:', error.message);

    // Generic error message (don't expose internal details)
    res.status(500).json({
      success: false,
      error: 'AI request failed',
      message: 'Please try again later'
    });
  }
}

module.exports = handleRequest;
