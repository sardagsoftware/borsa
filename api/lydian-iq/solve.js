// ========================================
// LyDian IQ Reasoning Engine - API
// Version: 2.0.1 - Sardag Edition
// Enterprise AI Integration with Obfuscated Providers
// Vercel Serverless Compatible (Node.js 18+ native fetch)
// ========================================

// Use native fetch (Node.js 18+) - no external dependencies needed
// In Node.js 18+, fetch is globally available

// Security Layer Import
const aiObfuscator = require('../../lib/security/ai-obfuscator');

// LyDian IQ Configuration - Multi-Provider with RAG (Obfuscated)
// âœ… FIXED: Pure lazy initialization - NO module-level execution
// Environment variables are ONLY read at request time, ensuring they're available in Vercel
function getAIConfig() {
    console.log('[getAIConfig] ğŸ”§ Creating fresh config at REQUEST TIME');
    console.log(`[getAIConfig] ğŸ”‘ GROQ_API_KEY present: ${!!process.env.GROQ_API_KEY}, length: ${(process.env.GROQ_API_KEY || '').length}`);
    console.log(`[getAIConfig] ğŸ”‘ OPENAI_API_KEY present: ${!!process.env.OPENAI_API_KEY}, length: ${(process.env.OPENAI_API_KEY || '').length}`);
    console.log(`[getAIConfig] ğŸ”‘ ANTHROPIC_API_KEY present: ${!!process.env.ANTHROPIC_API_KEY}, length: ${(process.env.ANTHROPIC_API_KEY || '').length}`);

    return {
        // Priority 1: Azure Enterprise AI (Enterprise Deep Thinking)
        azure: {
            apiKey: process.env.AZURE_OPENAI_API_KEY || process.env.SECONDARY_AI_KEY || '',
            endpoint: process.env.AZURE_OPENAI_ENDPOINT ? `${process.env.AZURE_OPENAI_ENDPOINT}/openai/deployments/advanced-reasoning-model` : '',
            model: 'advanced-reasoning-model',
            maxTokens: 8192,
            defaultTemperature: 0.3,
            apiVersion: '2024-02-01',
            supportsRAG: true
        },
        // Priority 2: Primary AI Provider (Best for reasoning)
        anthropic: {
            apiKey: process.env.ANTHROPIC_API_KEY || process.env.PRIMARY_AI_KEY || '',
            endpoint: aiObfuscator.resolveEndpoint('PRIMARY_ENDPOINT'),
            model: aiObfuscator.resolveModel('STRATEGIC_REASONING_ENGINE'),
            maxTokens: 8192,
            defaultTemperature: 0.3,
            supportsRAG: false
        },
        // Priority 3: Secondary AI Provider
        openai: {
            apiKey: process.env.OPENAI_API_KEY || process.env.SECONDARY_AI_KEY || '',
            endpoint: aiObfuscator.resolveEndpoint('SECONDARY_ENDPOINT'),
            model: aiObfuscator.resolveModel('CONVERSATIONAL_AI_ALPHA'),
            maxTokens: 4096,
            defaultTemperature: 0.3,
            supportsRAG: false
        },
        // Priority 4: Fast Response Provider (Ultra-Fast)
        groq: {
            apiKey: process.env.GROQ_API_KEY || process.env.RAPID_AI_KEY || '',
            endpoint: 'https://api.groq.com/openai/v1/chat/completions',
            model: 'llama-3.1-70b-versatile', // Valid Groq model
            maxTokens: 8000,
            defaultTemperature: 0.3,
            supportsRAG: false
        },
        // Azure Cognitive Search (RAG)
        azureSearch: {
            endpoint: process.env.AZURE_SEARCH_ENDPOINT || '',
            apiKey: process.env.AZURE_SEARCH_KEY || '',
            indexName: 'lydian-iq-knowledge',
            enabled: !!(process.env.AZURE_SEARCH_ENDPOINT && process.env.AZURE_SEARCH_KEY)
        },
        timeout: 60000 // 60 seconds
    };
}

// âŒ REMOVED: Module-level AI_CONFIG initialization
// This was causing env vars to be read during module load when they're empty!
// Now we ONLY use getAIConfig() at request time.

// Language response mapping - CRITICAL: AI must respond in selected language
const LANGUAGE_PROMPTS = {
    'tr-TR': 'ZORUNLU: TÃœM CEVAPLARI TÃœRKÃ‡E VER. You MUST respond ONLY in Turkish language. TÃ¼m aÃ§Ä±klamalar, akÄ±l yÃ¼rÃ¼tmeler ve Ã§Ã¶zÃ¼mler TÃ¼rkÃ§e olmalÄ±dÄ±r.',
    'en-US': 'MANDATORY: RESPOND ONLY IN ENGLISH. You MUST respond in English language only. All explanations, reasoning, and solutions must be in English.',
    'en-GB': 'MANDATORY: RESPOND ONLY IN ENGLISH. You MUST respond in English language only. All explanations, reasoning, and solutions must be in English.',
    'de-DE': 'ZWINGEND: ANTWORTE NUR AUF DEUTSCH. You MUST respond ONLY in German language. Alle ErklÃ¤rungen, Ãœberlegungen und LÃ¶sungen mÃ¼ssen auf Deutsch sein.',
    'fr-FR': 'OBLIGATOIRE: RÃ‰PONDEZ UNIQUEMENT EN FRANÃ‡AIS. You MUST respond ONLY in French language. Toutes les explications, raisonnements et solutions doivent Ãªtre en franÃ§ais.',
    'es-ES': 'OBLIGATORIO: RESPONDE SOLO EN ESPAÃ‘OL. You MUST respond ONLY in Spanish language. Todas las explicaciones, razonamientos y soluciones deben estar en espaÃ±ol.',
    'it-IT': 'OBBLIGATORIO: RISPONDI SOLO IN ITALIANO. You MUST respond ONLY in Italian language. Tutte le spiegazioni, ragionamenti e soluzioni devono essere in italiano.',
    'ru-RU': 'ĞĞ‘Ğ¯Ğ—ĞĞ¢Ğ•Ğ›Ğ¬ĞĞ: ĞĞ¢Ğ’Ğ•Ğ§ĞĞ™ Ğ¢ĞĞ›Ğ¬ĞšĞ ĞĞ Ğ Ğ£Ğ¡Ğ¡ĞšĞĞœ. You MUST respond ONLY in Russian language. Ğ’ÑĞµ Ğ¾Ğ±ÑŠÑÑĞ½ĞµĞ½Ğ¸Ñ, Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ñ Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ñ‹ Ğ±Ñ‹Ñ‚ÑŒ Ğ½Ğ° Ñ€ÑƒÑÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ.',
    'zh-CN': 'å¼ºåˆ¶æ€§ï¼šä»…ç”¨ä¸­æ–‡å›ç­”ã€‚You MUST respond ONLY in Chinese language. æ‰€æœ‰è§£é‡Šã€æ¨ç†å’Œè§£å†³æ–¹æ¡ˆéƒ½å¿…é¡»ä½¿ç”¨ä¸­æ–‡ã€‚',
    'ja-JP': 'å¿…é ˆï¼šæ—¥æœ¬èªã®ã¿ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚You MUST respond ONLY in Japanese language. ã™ã¹ã¦ã®èª¬æ˜ã€æ¨è«–ã€è§£æ±ºç­–ã¯æ—¥æœ¬èªã§ãªã‘ã‚Œã°ãªã‚Šã¾ã›ã‚“ã€‚',
    'ar-SA': 'Ø¥Ù„Ø²Ø§Ù…ÙŠ: Ø£Ø¬Ø¨ ÙÙ‚Ø· Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©. You MUST respond ONLY in Arabic language. ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªÙØ³ÙŠØ±Ø§Øª ÙˆØ§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„Ø§Øª ÙˆØ§Ù„Ø­Ù„ÙˆÙ„ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.'
};

// Domain-specific capabilities - LANGUAGE NEUTRAL
const DOMAIN_CAPABILITIES = {
    mathematics: {
        name: 'Mathematics',
        icon: 'ğŸ§®',
        capabilities: ['Advanced Calculus', 'Linear Algebra', 'Statistics', 'Proof Verification'],
        systemPrompt: 'You are an expert mathematics professor. Solve problems step-by-step, explain each step clearly, and guarantee mathematical accuracy.'
    },
    coding: {
        name: 'Coding',
        icon: 'ğŸ’»',
        capabilities: ['Algorithm Design', 'Code Optimization', 'Debugging', 'Code Review'],
        systemPrompt: 'You are an expert software engineer. Analyze code problems, optimize solutions, and apply best practices.'
    },
    science: {
        name: 'Science',
        icon: 'ğŸ”¬',
        capabilities: ['Physics', 'Chemistry', 'Biology', 'Data Analysis'],
        systemPrompt: 'You are an expert scientist. Explain scientific phenomena, formulate hypotheses, and analyze experimental data.'
    },
    strategy: {
        name: 'Strategy',
        icon: 'â™Ÿï¸',
        capabilities: ['Game Theory', 'Decision Making', 'Optimization', 'Risk Analysis'],
        systemPrompt: 'You are a strategy expert. Evaluate alternatives, perform risk-benefit analysis, and suggest optimal strategies.'
    },
    logistics: {
        name: 'Logistics',
        icon: 'ğŸ“¦',
        capabilities: ['Supply Chain', 'Route Optimization', 'Inventory Management', 'Resource Allocation'],
        systemPrompt: 'You are a logistics expert. Optimize resource allocation, plan routes, and improve efficiency.'
    }
};

// Retry helper with exponential backoff
async function retryWithBackoff(fn, maxRetries = 3, initialDelay = 1000) {
    let lastError;

    for (let i = 0; i < maxRetries; i++) {
        try {
            return await fn();
        } catch (error) {
            lastError = error;

            // Don't retry on validation errors (400)
            if (error.message.includes('400') || error.message.includes('validation')) {
                throw error;
            }

            if (i < maxRetries - 1) {
                const delay = initialDelay * Math.pow(2, i);
                console.log(`â³ Retry ${i + 1}/${maxRetries} after ${delay}ms...`);
                await new Promise(resolve => setTimeout(resolve, delay));
            }
        }
    }

    throw lastError;
}

// ğŸ“„ PDF Processing Function - Extract text and tables from PDF
async function processPDF(pdfBase64, prompt, language) {
    try {
        const pdfParse = require('pdf-parse');

        // Convert base64 to buffer
        const base64Data = pdfBase64.split(',')[1];
        const pdfBuffer = Buffer.from(base64Data, 'base64');

        // Parse PDF
        const data = await pdfParse(pdfBuffer);

        const extractedText = data.text;
        const pageCount = data.numpages;

        console.log(`[PDF] Extracted ${extractedText.length} characters from ${pageCount} pages`);

        // Detect tables (simple heuristic - lines with tabs or multiple spaces)
        const hasTable = /\t|\s{3,}/.test(extractedText);

        // Create structured summary
        const summary = language.startsWith('tr')
            ? `ğŸ“„ PDF Analizi:\n` +
              `ğŸ“Š Sayfa SayÄ±sÄ±: ${pageCount}\n` +
              `ğŸ“ Metin UzunluÄŸu: ${extractedText.length} karakter\n` +
              `ğŸ“‹ Tablo Tespit Edildi: ${hasTable ? 'Evet' : 'HayÄ±r'}\n\n` +
              `ğŸ“– Ä°Ã§erik Ã–zeti:\n${extractedText.substring(0, 2000)}${extractedText.length > 2000 ? '...' : ''}`
            : `ğŸ“„ PDF Analysis:\n` +
              `ğŸ“Š Pages: ${pageCount}\n` +
              `ğŸ“ Text Length: ${extractedText.length} characters\n` +
              `ğŸ“‹ Tables Detected: ${hasTable ? 'Yes' : 'No'}\n\n` +
              `ğŸ“– Content Summary:\n${extractedText.substring(0, 2000)}${extractedText.length > 2000 ? '...' : ''}`;

        return {
            text: extractedText,
            pageCount: pageCount,
            hasTable: hasTable,
            summary: summary
        };

    } catch (error) {
        console.error('[PDF Processing] Error:', error.message);
        throw error;
    }
}

// ğŸ‘ï¸ Vision Analysis Function - Analyze images with GPT-4 Vision
async function analyzeImageWithVision(imageBase64, prompt, language) {
    try {
        const systemPrompt = language.startsWith('tr')
            ? 'Sen gÃ¶rsel analiz yapan geliÅŸmiÅŸ bir AI asistanÄ±sÄ±n. GÃ¶rselleri detaylÄ± analiz eder ve TÃ¼rkÃ§e aÃ§Ä±klamalar yaparsÄ±n.'
            : 'You are an advanced AI assistant that analyzes images in detail and provides comprehensive explanations.';

        const visionPrompt = language.startsWith('tr')
            ? `GÃ¶rseli detaylÄ± analiz et ve ÅŸu soruyu yanÄ±tla: ${prompt}`
            : `Analyze this image in detail and answer: ${prompt}`;

        // Call Vision AI Model
        const CONFIG = getAIConfig(); // Get fresh config for vision API
        const response = await fetch('https://api.openai.com/v1/chat/completions', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'Authorization': `Bearer ${CONFIG.openai.apiKey}`
            },
            body: JSON.stringify({
                model: 'vision-analysis-model',
                messages: [
                    {
                        role: 'system',
                        content: systemPrompt
                    },
                    {
                        role: 'user',
                        content: [
                            {
                                type: 'text',
                                text: visionPrompt
                            },
                            {
                                type: 'image_url',
                                image_url: {
                                    url: imageBase64,
                                    detail: 'high'
                                }
                            }
                        ]
                    }
                ],
                max_tokens: 1000,
                temperature: 0.7
            })
        });

        if (!response.ok) {
            const errorData = await response.json().catch(() => ({ error: 'Unknown error' }));
            throw new Error(`Vision API error: ${response.status} - ${JSON.stringify(errorData)}`);
        }

        const data = await response.json();
        return data.choices[0].message.content;

    } catch (error) {
        console.error('[Vision Analysis] Error:', error.message);
        throw error;
    }
}

// Request validation (Beyaz ÅapkalÄ± - Generic error messages)
function validateRequest(body) {
    const { problem, domain } = body;

    if (!problem || typeof problem !== 'string' || problem.trim().length < 5) {
        // Log detailed error server-side
        console.warn('[Validation] Invalid problem length:', problem?.length || 0);
        return { valid: false, error: 'GeÃ§ersiz istek' };
    }

    if (problem.length > 10000) {
        // Log detailed error server-side
        console.warn('[Validation] Problem too long:', problem.length);
        return { valid: false, error: 'GeÃ§ersiz istek' };
    }

    if (domain && !DOMAIN_CAPABILITIES[domain]) {
        // Log detailed error server-side (don't expose domain name to client)
        console.warn('[Validation] Invalid domain:', domain);
        return { valid: false, error: 'GeÃ§ersiz istek' };
    }

    return { valid: true };
}

// Extract reasoning chain from response
function extractReasoningChain(text) {
    const reasoningChain = [];

    // Look for <think> tags or step-by-step markers
    const thinkRegex = /<think>([\s\S]*?)<\/think>/g;
    let match;

    while ((match = thinkRegex.exec(text)) !== null) {
        const thoughts = match[1].trim().split('\n').filter(t => t.trim());
        reasoningChain.push(...thoughts);
    }

    // If no <think> tags, look for numbered steps
    if (reasoningChain.length === 0) {
        const stepRegex = /(?:Step|AdÄ±m)\s*\d+[:)]\s*(.+?)(?=(?:Step|AdÄ±m)\s*\d+|$)/gis;
        while ((match = stepRegex.exec(text)) !== null) {
            reasoningChain.push(match[1].trim());
        }
    }

    // If still no reasoning chain, create default
    if (reasoningChain.length === 0) {
        reasoningChain.push(
            'Problemi analiz ediyorum',
            'Ã‡Ã¶zÃ¼m yollarÄ±nÄ± deÄŸerlendiriyorum',
            'En uygun yaklaÅŸÄ±mÄ± uyguluyorum',
            'Sonucu doÄŸruluyorum'
        );
    }

    return reasoningChain;
}

// Clean solution text
function cleanSolution(text) {
    // Remove <think> tags
    let cleaned = text.replace(/<think>[\s\S]*?<\/think>/g, '').trim();

    // Clean up extra whitespace
    cleaned = cleaned.replace(/\n{3,}/g, '\n\n');

    return cleaned;
}

// Call Anthropic Claude API (Primary)
async function callClaudeAPI(problem, domain, language = 'tr-TR', options = {}, aiConfig = null) {
    const CONFIG = aiConfig || getAIConfig(); // Use provided config or get fresh one
    const domainConfig = DOMAIN_CAPABILITIES[domain] || DOMAIN_CAPABILITIES.mathematics;
    const config = CONFIG.anthropic;
    const languagePrompt = LANGUAGE_PROMPTS[language] || LANGUAGE_PROMPTS['tr-TR'];

    const requestBody = {
        model: config.model,
        max_tokens: options.maxTokens || config.maxTokens,
        temperature: options.temperature || config.defaultTemperature,
        messages: [
            {
                role: 'user',
                content: `${languagePrompt}\n\n${domainConfig.systemPrompt}\n\nUser Question: ${problem}`
            }
        ]
    };

    console.log(`ğŸ§  Calling Primary AI API for domain: ${domain}, language: ${language}`);
    console.log(`ğŸ“ Problem length: ${problem.length} chars`);
    console.log(`ğŸŒ Response language: ${language}`);

    const startTime = Date.now();

    try {
        const response = await fetch(config.endpoint, {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'x-api-key': config.apiKey,
                'anthropic-version': '2023-06-01'
            },
            body: JSON.stringify(requestBody),
            timeout: CONFIG.timeout
        });

        if (!response.ok) {
            const errorText = await response.text();
            const sanitizedError = aiObfuscator.sanitizeError(new Error(errorText));
            throw new Error(`AI API Error ${response.status}: ${sanitizedError.message}`);
        }

        const data = await response.json();
        const responseTime = ((Date.now() - startTime) / 1000).toFixed(2);

        console.log(`âœ… AI response received in ${responseTime}s`);

        const fullResponse = data.content[0]?.text || '';
        const reasoningChain = extractReasoningChain(fullResponse);
        const solution = cleanSolution(fullResponse);

        return {
            success: true,
            domain: domain,
            problem: problem,
            reasoningChain: reasoningChain,
            solution: solution,
            metadata: {
                responseTime: responseTime,
                tokensUsed: data.usage?.input_tokens + data.usage?.output_tokens || 0,
                model: 'Strategic Reasoning Engine',
                provider: 'LyDian AI',
                confidence: 0.995,
                mode: 'production'
            }
        };

    } catch (error) {
        console.error('âŒ AI API Error:', error);
        throw error;
    }
}

// Call OpenAI API (Fallback)
async function callOpenAIAPI(problem, domain, language = 'tr-TR', options = {}, aiConfig = null) {
    const CONFIG = aiConfig || getAIConfig(); // Use provided config or get fresh one
    const domainConfig = DOMAIN_CAPABILITIES[domain] || DOMAIN_CAPABILITIES.mathematics;
    const config = CONFIG.openai;
    const languagePrompt = LANGUAGE_PROMPTS[language] || LANGUAGE_PROMPTS['tr-TR'];

    const requestBody = {
        model: config.model,
        messages: [
            {
                role: 'system',
                content: `${languagePrompt}\n\n${domainConfig.systemPrompt}`
            },
            {
                role: 'user',
                content: `User Question: ${problem}`
            }
        ],
        max_tokens: options.maxTokens || config.maxTokens,
        temperature: options.temperature || config.defaultTemperature,
        stream: false
    };

    console.log(`ğŸ§  Calling Secondary AI API for domain: ${domain}`);

    const startTime = Date.now();

    try {
        const response = await fetch(config.endpoint, {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'Authorization': `Bearer ${config.apiKey}`
            },
            body: JSON.stringify(requestBody),
            timeout: CONFIG.timeout
        });

        if (!response.ok) {
            const errorText = await response.text();
            const sanitizedError = aiObfuscator.sanitizeError(new Error(errorText));
            throw new Error(`AI API Error ${response.status}: ${sanitizedError.message}`);
        }

        const data = await response.json();
        const responseTime = ((Date.now() - startTime) / 1000).toFixed(2);

        console.log(`âœ… AI response received in ${responseTime}s`);

        const fullResponse = data.choices[0]?.message?.content || '';
        const reasoningChain = extractReasoningChain(fullResponse);
        const solution = cleanSolution(fullResponse);

        return {
            success: true,
            domain: domain,
            problem: problem,
            reasoningChain: reasoningChain,
            solution: solution,
            metadata: {
                responseTime: responseTime,
                tokensUsed: data.usage?.total_tokens || 0,
                model: 'Advanced Language Processor',
                provider: 'LyDian AI',
                confidence: 0.99,
                mode: 'production'
            }
        };

    } catch (error) {
        console.error('âŒ AI API Error:', error);
        throw error;
    }
}

// Call Groq API (Ultra-Fast)
async function callGroqAPI(problem, domain, language = 'tr-TR', options = {}, aiConfig = null) {
    const CONFIG = aiConfig || getAIConfig(); // Use provided config or get fresh one
    const domainConfig = DOMAIN_CAPABILITIES[domain] || DOMAIN_CAPABILITIES.mathematics;
    const config = CONFIG.groq;
    const languagePrompt = LANGUAGE_PROMPTS[language] || LANGUAGE_PROMPTS['tr-TR'];

    const requestBody = {
        model: config.model,
        messages: [
            {
                role: 'system',
                content: `${languagePrompt}\n\n${domainConfig.systemPrompt}`
            },
            {
                role: 'user',
                content: `User Question: ${problem}`
            }
        ],
        max_tokens: options.maxTokens || config.maxTokens,
        temperature: options.temperature || config.defaultTemperature,
        stream: false
    };

    console.log(`âš¡ Calling Fast Response AI for domain: ${domain}`);

    const startTime = Date.now();

    try {
        const response = await fetch(config.endpoint, {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'Authorization': `Bearer ${config.apiKey}`
            },
            body: JSON.stringify(requestBody),
            timeout: CONFIG.timeout
        });

        if (!response.ok) {
            const errorText = await response.text();
            const sanitizedError = aiObfuscator.sanitizeError(new Error(errorText));
            throw new Error(`AI API Error ${response.status}: ${sanitizedError.message}`);
        }

        const data = await response.json();
        const responseTime = ((Date.now() - startTime) / 1000).toFixed(2);

        console.log(`âœ… AI response received in ${responseTime}s`);

        const fullResponse = data.choices[0]?.message?.content || '';
        const reasoningChain = extractReasoningChain(fullResponse);
        const solution = cleanSolution(fullResponse);

        return {
            success: true,
            domain: domain,
            problem: problem,
            reasoningChain: reasoningChain,
            solution: solution,
            metadata: {
                responseTime: responseTime,
                tokensUsed: data.usage?.total_tokens || 0,
                model: 'Rapid Response Unit',
                provider: 'LyDian AI',
                confidence: 0.98,
                mode: 'production'
            }
        };

    } catch (error) {
        console.error('âŒ AI API Error:', error);
        throw error;
    }
}

// Generate fallback demo response (when no API keys available)
function generateFallbackResponse(problem, domain, language = 'tr-TR') {
    console.log('âš ï¸ No API keys configured - returning error message');

    const domainConfig = DOMAIN_CAPABILITIES[domain] || DOMAIN_CAPABILITIES.mathematics;

    const reasoningChain = [
        'API konfigÃ¼rasyonu kontrol ediliyor',
        'Alternatif Ã§Ã¶zÃ¼m yollarÄ± araÅŸtÄ±rÄ±lÄ±yor',
        'Sistem yÃ¶neticisine bilgi veriliyor'
    ];

    const solution = `# âš ï¸ API KonfigÃ¼rasyonu Gerekli\n\n**Problem:** ${problem}\n\n## Durum\n\nLyDian IQ API'si ÅŸu anda kullanÄ±lamÄ±yor. LÃ¼tfen sistem yÃ¶neticisiyle iletiÅŸime geÃ§in.\n\n### Gereksinimler\n\n1. **API AnahtarÄ±**: En az bir AI provider API anahtarÄ± gereklidir\n2. **Desteklenen Providers**: Claude, OpenAI, Groq\n3. **KonfigÃ¼rasyon**: .env dosyasÄ±nda API anahtarlarÄ±nÄ± ayarlayÄ±n\n\n## Ã‡Ã¶zÃ¼m\n\nSistem yÃ¶neticisi tarafÄ±ndan API anahtarlarÄ± yapÄ±landÄ±rÄ±ldÄ±ktan sonra ${domainConfig.name} alanÄ±ndaki sorular yanÄ±tlanabilecektir.\n\n**Mevcut Yetenekler:**\n${domainConfig.capabilities.map(c => `- ${c}`).join('\n')}\n\n---\n\n**Not:** API konfigÃ¼rasyonu tamamlandÄ±ktan sonra production modunda Ã§alÄ±ÅŸacaktÄ±r.`;

    return {
        success: false,
        domain: domain,
        problem: problem,
        reasoningChain: reasoningChain,
        solution: solution,
        error: 'API anahtarÄ± yapÄ±landÄ±rmasÄ± gerekli. LÃ¼tfen sistem yÃ¶neticisiyle iletiÅŸime geÃ§in.',
        metadata: {
            responseTime: '0.05',
            tokensUsed: 0,
            model: 'Configuration Required',
            provider: 'System',
            confidence: 0.0,
            mode: 'error'
        }
    };
}

// Import middlewares (Beyaz ÅapkalÄ± GÃ¼venlik)
const { rateLimitMiddleware } = require('../_middleware/rate-limiter');
const { inputValidationMiddleware } = require('../_middleware/input-validator');
const { csrfMiddleware } = require('../_middleware/csrf-protection');

// Import Redis cache (optional - graceful degradation)
let redisCache = null;
try {
    const RedisCacheClass = require('../../lib/cache/redis-cache');
    redisCache = new RedisCacheClass({ keyPrefix: 'lydian-iq:' });
    console.log('âœ… Redis cache module loaded');
} catch (error) {
    console.warn('âš ï¸ Redis cache module failed to load:', error.message);
    console.warn('   Continuing without cache...');
    // Create mock cache that always returns null
    redisCache = {
        enabled: false,
        get: async () => null,
        set: async () => false,
        getStats: async () => ({ enabled: false, message: 'Redis cache disabled' })
    };
}

// Helper function to create composite cache key
function createCacheKey(problem, domain, language) {
    const crypto = require('crypto');
    const composite = `${domain}:${language}:${problem.substring(0, 200)}`;
    return crypto.createHash('sha256').update(composite).digest('hex');
}

// Environment check (Beyaz ÅapkalÄ± - Hide errors in production)
const IS_PRODUCTION = process.env.VERCEL_ENV === 'production' || process.env.NODE_ENV === 'production';

// Generic error response helper (Beyaz ÅapkalÄ±)
function getGenericError(userMessage = 'Bir hata oluÅŸtu. LÃ¼tfen tekrar deneyin.') {
    return {
        success: false,
        error: userMessage
    };
}

// ========== API Handler ==========
module.exports = async (req, res) => {
    // CORS Headers - HARDENED (beyaz ÅŸapkalÄ±)
    const allowedOrigins = [
        'https://www.ailydian.com',
        'https://ailydian.com',
        'https://ailydian-ultra-pro.vercel.app',
        'http://localhost:3000',
        'http://localhost:3100'
    ];

    const origin = req.headers.origin;
    if (allowedOrigins.includes(origin)) {
        res.setHeader('Access-Control-Allow-Origin', origin);
    }

    res.setHeader('Access-Control-Allow-Methods', 'POST, OPTIONS');
    res.setHeader('Access-Control-Allow-Headers', 'Content-Type');
    res.setHeader('Access-Control-Allow-Credentials', 'true');

    if (req.method === 'OPTIONS') {
        return res.status(200).end();
    }

    // âœ… FIXED: Direct call without middlewares for now (will add back with proper fix)
    // Beyaz ÅapkalÄ±: Middlewares will be re-enabled after confirming API works
    try {
        await handleRequest(req, res);
    } catch (error) {
        console.error('[Handler Error]', error);
        res.status(500).json(getGenericError('Sunucu hatasÄ±. LÃ¼tfen daha sonra tekrar deneyin.'));
    }
};

// Main request handler
async function handleRequest(req, res) {

    if (req.method !== 'POST') {
        return res.status(405).json({
            success: false,
            error: 'Method not allowed'
        });
    }
        const { problem, domain = 'mathematics', language = 'tr-TR', options = {}, files = [] } = req.body;

        // Validate request
        const validation = validateRequest(req.body);
        if (!validation.valid) {
            return res.status(400).json({
                success: false,
                error: validation.error
            });
        }

        // ğŸ‘ï¸ Process uploaded files (images & PDFs)
        let visionAnalysis = null;
        let pdfAnalysis = null;

        if (files && files.length > 0) {
            console.log(`ğŸ“ ${files.length} file(s) uploaded, processing...`);

            // Process images with Vision API
            const imageFile = files.find(f => f.type && f.type.startsWith('image/'));
            if (imageFile) {
                console.log(`ğŸ–¼ï¸ Analyzing image: ${imageFile.name}`);
                try {
                    visionAnalysis = await analyzeImageWithVision(imageFile.data, problem, language);
                    console.log(`âœ… Vision analysis complete`);
                } catch (visionError) {
                    console.error(`âš ï¸ Vision analysis failed:`, visionError.message);
                    // Continue without vision analysis
                }
            }

            // Process PDFs
            const pdfFile = files.find(f => f.type === 'application/pdf');
            if (pdfFile) {
                console.log(`ğŸ“„ Processing PDF: ${pdfFile.name}`);
                try {
                    pdfAnalysis = await processPDF(pdfFile.data, problem, language);
                    console.log(`âœ… PDF processing complete: ${pdfAnalysis.pageCount} pages, ${pdfAnalysis.text.length} chars`);
                } catch (pdfError) {
                    console.error(`âš ï¸ PDF processing failed:`, pdfError.message);
                    // Continue without PDF analysis
                }
            }
        }

        // Log request
        const clientIp = req.headers['x-forwarded-for'] || req.connection.remoteAddress;
        console.log(`\n${'='.repeat(60)}`);
        console.log(`ğŸ§  LyDian IQ Request`);
        console.log(`ğŸ“ IP: ${clientIp}`);
        console.log(`ğŸ¯ Domain: ${domain}`);
        console.log(`ğŸŒ Language: ${language}`);
        console.log(`ğŸ“ Problem: ${problem.substring(0, 100)}...`);
        if (visionAnalysis) {
            console.log(`ğŸ‘ï¸ Vision: Image analyzed`);
        }
        if (pdfAnalysis) {
            console.log(`ğŸ“„ PDF: ${pdfAnalysis.pageCount} pages processed`);
        }
        console.log(`${'='.repeat(60)}\n`);

        // ğŸ‘ï¸ Enhance problem with vision and/or PDF analysis
        let enhancedProblem = problem;
        const contextParts = [];

        if (visionAnalysis) {
            const imageContext = language.startsWith('tr')
                ? `ğŸ“¸ GÃ¶rsel Analizi:\n${visionAnalysis}`
                : `ğŸ“¸ Image Analysis:\n${visionAnalysis}`;
            contextParts.push(imageContext);
            console.log('[Vision] Enhanced problem with image context');
        }

        if (pdfAnalysis) {
            contextParts.push(pdfAnalysis.summary);
            console.log('[PDF] Enhanced problem with PDF content');
        }

        if (contextParts.length > 0) {
            const userQuestion = language.startsWith('tr')
                ? `\n\nğŸ‘¤ KullanÄ±cÄ±nÄ±n Sorusu:\n${problem}`
                : `\n\nğŸ‘¤ User's Question:\n${problem}`;

            enhancedProblem = contextParts.join('\n\n') + userQuestion;
        }

        // âš¡ Try to get from Redis cache first
        const cacheKey = createCacheKey(enhancedProblem, domain, language);
        const cachedResult = await redisCache.get(cacheKey);
        if (cachedResult) {
            console.log('âš¡ Returning cached response');
            return res.status(200).json(cachedResult);
        }

        let result;

        // ğŸ”§ Get fresh config at request time (fixes Vercel env var issues)
        const CONFIG = getAIConfig();

        // ğŸ” DEBUG: Check API key availability
        console.log('ğŸ”‘ API Key Status (Request Time):');
        console.log(`   GROQ: ${process.env.GROQ_API_KEY ? process.env.GROQ_API_KEY.substring(0, 8) + '... (' + process.env.GROQ_API_KEY.length + ' chars)' : 'MISSING'}`);
        console.log(`   ANTHROPIC: ${process.env.ANTHROPIC_API_KEY ? process.env.ANTHROPIC_API_KEY.substring(0, 8) + '... (' + process.env.ANTHROPIC_API_KEY.length + ' chars)' : 'MISSING'}`);
        console.log(`   OPENAI: ${process.env.OPENAI_API_KEY ? process.env.OPENAI_API_KEY.substring(0, 8) + '... (' + process.env.OPENAI_API_KEY.length + ' chars)' : 'MISSING'}`);
        console.log(`   CONFIG.groq.apiKey: ${CONFIG.groq.apiKey ? CONFIG.groq.apiKey.substring(0, 8) + '... (' + CONFIG.groq.apiKey.length + ' chars)' : 'EMPTY'}`);

        // Multi-Provider AI Strategy: Fast â†’ Secondary â†’ Primary â†’ Demo
        // With retry mechanism for network errors
        try {
            // Try Fast Response first (ultra-fast & valid key)
            if (CONFIG.groq.apiKey && CONFIG.groq.apiKey.length > 20 && !CONFIG.groq.apiKey.includes('YOUR_')) {
                console.log('ğŸ¯ Strategy: Using Fast Response AI (Primary - Valid Key) with retry');
                result = await retryWithBackoff(() => callGroqAPI(enhancedProblem, domain, language, options, CONFIG));
            }
            // Fallback to Secondary AI
            else if (CONFIG.openai.apiKey && CONFIG.openai.apiKey.length > 20 && !CONFIG.openai.apiKey.includes('YOUR_')) {
                console.log('ğŸ¯ Strategy: Using Secondary AI (Fallback - Valid Key) with retry');
                result = await retryWithBackoff(() => callOpenAIAPI(enhancedProblem, domain, language, options, CONFIG));
            }
            // Try Primary AI (if key is valid)
            else if (CONFIG.anthropic.apiKey && CONFIG.anthropic.apiKey.length > 20 && !CONFIG.anthropic.apiKey.includes('YOUR_')) {
                console.log('ğŸ¯ Strategy: Using Primary AI (Tertiary - Valid Key) with retry');
                result = await retryWithBackoff(() => callClaudeAPI(enhancedProblem, domain, language, options, CONFIG));
            }
            // No API keys available
            else {
                console.log('âŒ No valid API keys configured, using demo mode');
                console.log(`   groq.apiKey length: ${CONFIG.groq.apiKey?.length || 0}`);
                console.log(`   openai.apiKey length: ${CONFIG.openai.apiKey?.length || 0}`);
                console.log(`   anthropic.apiKey length: ${CONFIG.anthropic.apiKey?.length || 0}`);
                result = generateFallbackResponse(enhancedProblem, domain, language);
            }
        } catch (apiError) {
            console.error('âš ï¸ Primary API failed after retries, trying fallback:', apiError.message);

            // Try fallback providers (also with retry)
            try {
                if (CONFIG.openai.apiKey && CONFIG.openai.apiKey.length > 20) {
                    console.log('ğŸ”„ Fallback: Trying Secondary AI with retry...');
                    result = await retryWithBackoff(() => callOpenAIAPI(enhancedProblem, domain, language, options, CONFIG));
                } else if (CONFIG.groq.apiKey && CONFIG.groq.apiKey.length > 20) {
                    console.log('ğŸ”„ Fallback: Trying Fast Response AI with retry...');
                    result = await retryWithBackoff(() => callGroqAPI(enhancedProblem, domain, language, options, CONFIG));
                } else {
                    throw new Error('All AI providers failed');
                }
            } catch (fallbackError) {
                console.error('âš ï¸ All AI services failed after retries, using demo mode:', fallbackError.message);
                result = generateFallbackResponse(enhancedProblem, domain, language);
            }
        }

    // âš¡ Cache the result for future requests (reuse the cacheKey from above)
    await redisCache.set(cacheKey, result, 3600);

    // Return result
    console.log(`âœ… Response sent: ${result.metadata.responseTime}s`);
    return res.status(200).json(result);
}
