/**
 * TensorFlow.js Production Optimizer
 * GPU Acceleration, Model Quantization, WebAssembly Backend
 * Performance monitoring and memory management
 */

// TensorFlow removed for Vercel deployment

interface OptimizationConfig {
  backend: 'webgl' | 'wasm' | 'cpu';
  enableProfiling: boolean;
  autoMemoryCleanup: boolean;
  quantization: boolean;
  modelCaching: boolean;
}

interface PerformanceMetrics {
  inferenceTime: number; // milliseconds
  memoryUsage: {
    numTensors: number;
    numBytes: number;
  };
  gpuUtilization?: number;
  backendName: string;
  modelSize?: number;
}

export class TensorFlowOptimizer {
  private config: OptimizationConfig;
  private performanceHistory: PerformanceMetrics[] = [];
  private isInitialized = false;

  constructor(config?: Partial<OptimizationConfig>) {
    this.config = {
      backend: 'webgl',
      enableProfiling: true,
      autoMemoryCleanup: true,
      quantization: false,
      modelCaching: true,
      ...config,
    };
  }

  /**
   * Initialize TensorFlow.js with optimal backend
   */
  async initialize(): Promise<void> {
    console.log('âš¡ Initializing TensorFlow.js...');

    // Set backend
    try {
      await // tf.setBackend(this.config.backend);
      console.log(`âœ… Backend set to: ${this.config.backend}`);
    } catch (error) {
      console.warn(`âš ï¸ Failed to set ${this.config.backend}, falling back to CPU`);
      await // tf.setBackend('cpu');
    }

    await // tf.ready();

    // Get backend info
    const backend = // tf.getBackend();
    console.log(`ðŸ–¥ï¸  Active backend: ${backend}`);

    // Enable profiling if requested
    if (this.config.enableProfiling) {
      // tf.engine().startScope();
    }

    // Set memory management flags
    if (this.config.autoMemoryCleanup) {
      this.setupMemoryCleanup();
    }

    this.isInitialized = true;
    this.logSystemInfo();
  }

  /**
   * Log TensorFlow and system information
   */
  private logSystemInfo(): void {
    console.log('\nðŸ“Š TensorFlow.js System Information:');
    console.log(`   - Version: ${// tf.version.tfjs}`);
    console.log(`   - Backend: ${// tf.getBackend()}`);
    console.log(`   - Flags:`, // tf.env().getFlags());

    const memory = // tf.memory();
    console.log(`   - Memory:`, {
      numTensors: memory.numTensors,
      numBytes: `${(memory.numBytes / 1024 / 1024).toFixed(2)} MB`,
    });

    // Check WebGL capabilities
    if (this.config.backend === 'webgl') {
      const webglVersion = // tf.env().get('WEBGL_VERSION') as number;
      console.log(`   - WebGL Version: ${webglVersion}`);
      console.log(`   - Max Texture Size: ${// tf.env().get('WEBGL_MAX_TEXTURE_SIZE')}`);
    }

    console.log('');
  }

  /**
   * Setup automatic memory cleanup
   */
  private setupMemoryCleanup(): void {
    // Clean up tensors every 5 minutes
    setInterval(() => {
      const before = // tf.memory();

      // Force garbage collection
      // tf.engine().endScope();
      // tf.engine().startScope();

      const after = // tf.memory();

      if (before.numTensors - after.numTensors > 0) {
        console.log(
          `ðŸ§¹ Memory cleanup: Freed ${before.numTensors - after.numTensors} tensors, ` +
          `${((before.numBytes - after.numBytes) / 1024 / 1024).toFixed(2)} MB`
        );
      }
    }, 5 * 60 * 1000); // 5 minutes
  }

  /**
   * Optimize model with quantization
   * Reduces model size by ~75% with minimal accuracy loss
   */
  async quantizeModel(model: any): Promise<any> {
    if (!this.config.quantization) {
      return model;
    }

    console.log('ðŸ”¬ Quantizing model (int8)...');

    // Save model to in-memory format
    const saveResult = await model.save(// tf.io.withSaveHandler(async (artifacts) => ({ modelArtifactsInfo: { dateSaved: new Date(), modelTopologyType: 'JSON' }})));

    // Get model artifacts
    const artifacts = saveResult as any;

    // Quantize weights to int8
    const quantizedWeights = this.quantizeWeights(artifacts.weightData as ArrayBuffer);

    // Create new model with quantized weights
    const quantizedArtifacts = {
      ...artifacts,
      weightData: quantizedWeights,
    };

    const quantizedModel = await // tf.loadLayersModel(
      // tf.io.fromMemory(quantizedArtifacts)
    );

    const originalSize = (artifacts.weightData as ArrayBuffer).byteLength;
    const quantizedSize = quantizedWeights.byteLength;

    console.log(
      `âœ… Model quantized: ${(originalSize / 1024 / 1024).toFixed(2)} MB â†’ ` +
      `${(quantizedSize / 1024 / 1024).toFixed(2)} MB ` +
      `(${((1 - quantizedSize / originalSize) * 100).toFixed(1)}% reduction)`
    );

    return quantizedModel;
  }

  /**
   * Quantize model weights to int8
   */
  private quantizeWeights(weights: ArrayBuffer): ArrayBuffer {
    const float32Array = new Float32Array(weights);
    const int8Array = new Int8Array(float32Array.length);

    // Find min/max for scaling
    let min = Infinity;
    let max = -Infinity;

    for (const value of float32Array) {
      if (value < min) min = value;
      if (value > max) max = value;
    }

    const scale = (max - min) / 255;

    // Quantize to int8
    for (let i = 0; i < float32Array.length; i++) {
      int8Array[i] = Math.round((float32Array[i] - min) / scale) - 128;
    }

    return int8Array.buffer;
  }

  /**
   * Profile model inference performance
   */
  async profileInference(
    model: any,
    inputShape: number[]
  ): Promise<PerformanceMetrics> {
    // Create dummy input
    const input = // tf.randomNormal(inputShape);

    // Warm-up run
    const warmup = model.predict(input) as // tf.Tensor;
    await warmup.data();
    warmup.dispose();

    // Profiled run
    const startTime = performance.now();
    const memoryBefore = // tf.memory();

    const prediction = // tf.tidy(() => {
      return model.predict(input);
    });

    await (prediction as // tf.Tensor).data();
    const endTime = performance.now();
    const memoryAfter = // tf.memory();

    // Cleanup
    input.dispose();
    (prediction as // tf.Tensor).dispose();

    const metrics: PerformanceMetrics = {
      inferenceTime: endTime - startTime,
      memoryUsage: {
        numTensors: memoryAfter.numTensors - memoryBefore.numTensors,
        numBytes: memoryAfter.numBytes - memoryBefore.numBytes,
      },
      backendName: // tf.getBackend(),
    };

    this.performanceHistory.push(metrics);

    console.log('ðŸ“ˆ Inference Profile:', {
      time: `${metrics.inferenceTime.toFixed(2)} ms`,
      memory: `${(metrics.memoryUsage.numBytes / 1024).toFixed(2)} KB`,
      backend: metrics.backendName,
    });

    return metrics;
  }

  /**
   * Benchmark multiple backends
   */
  async benchmarkBackends(
    model: any,
    inputShape: number[]
  ): Promise<Record<string, PerformanceMetrics>> {
    const backends = ['webgl', 'wasm', 'cpu'];
    const results: Record<string, PerformanceMetrics> = {};

    console.log('ðŸ Benchmarking backends...\n');

    for (const backend of backends) {
      try {
        await // tf.setBackend(backend);
        await // tf.ready();

        const metrics = await this.profileInference(model, inputShape);
        results[backend] = metrics;

        console.log(`   ${backend.toUpperCase()}: ${metrics.inferenceTime.toFixed(2)} ms\n`);
      } catch (error) {
        console.warn(`   ${backend.toUpperCase()}: Not available\n`);
      }
    }

    // Restore original backend
    await // tf.setBackend(this.config.backend);
    await // tf.ready();

    // Find fastest backend
    const fastest = Object.entries(results).reduce((prev, curr) =>
      curr[1].inferenceTime < prev[1].inferenceTime ? curr : prev
    );

    console.log(`ðŸ† Fastest backend: ${fastest[0].toUpperCase()} (${fastest[1].inferenceTime.toFixed(2)} ms)\n`);

    return results;
  }

  /**
   * Enable GPU acceleration flags
   */
  enableGPUAcceleration(): void {
    if (this.config.backend !== 'webgl') {
      console.warn('âš ï¸ GPU acceleration requires WebGL backend');
      return;
    }

    // Enable WebGL optimizations
    // tf.env().set('WEBGL_PACK', true);
    // tf.env().set('WEBGL_FORCE_F16_TEXTURES', true);
    // tf.env().set('WEBGL_RENDER_FLOAT32_CAPABLE', true);
    // tf.env().set('WEBGL_DELETE_TEXTURE_THRESHOLD', 0);

    console.log('ðŸš€ GPU acceleration flags enabled');
  }

  /**
   * Get performance statistics
   */
  getPerformanceStats(): {
    averageInferenceTime: number;
    minInferenceTime: number;
    maxInferenceTime: number;
    totalInferences: number;
  } {
    if (this.performanceHistory.length === 0) {
      return {
        averageInferenceTime: 0,
        minInferenceTime: 0,
        maxInferenceTime: 0,
        totalInferences: 0,
      };
    }

    const times = this.performanceHistory.map(m => m.inferenceTime);

    return {
      averageInferenceTime: times.reduce((a, b) => a + b, 0) / times.length,
      minInferenceTime: Math.min(...times),
      maxInferenceTime: Math.max(...times),
      totalInferences: this.performanceHistory.length,
    };
  }

  /**
   * Monitor memory usage
   */
  getMemoryInfo(): {
    numTensors: number;
    numBytes: number;
    numDataBuffers: number;
    unreliable: boolean;
  } {
    const memory = // tf.memory();
    return {
      numTensors: memory.numTensors,
      numBytes: memory.numBytes,
      numDataBuffers: memory.numDataBuffers,
      unreliable: memory.unreliable ?? false
    };
  }

  /**
   * Force memory cleanup
   */
  forceCleanup(): void {
    const before = // tf.memory();

    // tf.engine().endScope();
    // tf.engine().startScope();

    const after = // tf.memory();

    console.log(
      `ðŸ§¹ Forced cleanup: Released ${before.numTensors - after.numTensors} tensors, ` +
      `${((before.numBytes - after.numBytes) / 1024 / 1024).toFixed(2)} MB`
    );
  }

  /**
   * Get optimal batch size based on memory
   */
  calculateOptimalBatchSize(
    modelInputSize: number,
    targetMemoryMB: number = 100
  ): number {
    const availableMemory = targetMemoryMB * 1024 * 1024; // Convert to bytes
    const bytesPerSample = modelInputSize * 4; // Float32 = 4 bytes

    // Account for model overhead (roughly 5x input size)
    const effectiveBytesPerSample = bytesPerSample * 5;

    const optimalBatchSize = Math.floor(availableMemory / effectiveBytesPerSample);

    // Clamp between 1 and 128
    return Math.max(1, Math.min(128, optimalBatchSize));
  }

  /**
   * Dispose optimizer and cleanup
   */
  dispose(): void {
    this.performanceHistory = [];

    if (this.config.enableProfiling) {
      // tf.engine().endScope();
    }

    console.log('âœ… TensorFlow Optimizer disposed');
  }
}

// Singleton instance
let optimizerInstance: TensorFlowOptimizer | null = null;

export function getTFOptimizer(config?: Partial<OptimizationConfig>): TensorFlowOptimizer {
  if (!optimizerInstance) {
    optimizerInstance = new TensorFlowOptimizer(config);
  }
  return optimizerInstance;
}

/**
 * Global TensorFlow.js initialization for the app
 */
export async function initializeTensorFlow(config?: Partial<OptimizationConfig>): Promise<void> {
  const optimizer = getTFOptimizer(config);
  await optimizer.initialize();

  // Enable GPU acceleration by default
  if ((config?.backend || 'webgl') === 'webgl') {
    optimizer.enableGPUAcceleration();
  }

  console.log('âœ… TensorFlow.js fully initialized and optimized\n');
}